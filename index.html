<!DOCTYPE html>
<html lang="en">
  <head>
    <br>
    <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta charset="utf-8">
    <title>Geometric Latent Augmentation for Shape Spaces</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
         <link rel="preconnect" href="https://fonts.googleapis.com"> 
         <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
         <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&display=swap" rel="stylesheet">
    <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/footable.standalone.min.css">

    <!-- Favicon
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="icon" type="image/png" href="images/favicon.png">

    <!-- Google icon -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

    <!-- Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                               m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-86869673-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
    <style>
      img {
          display: block;
      }

      .column-50 {
          float: left;
          width: 50%;
      }
      .row-50:after {
          content: "";
          display: table;
          clear: both;
      }

      .floating-teaser {
          float: left;
          width: 30%;
          text-align: center;
          padding: 15px;
      }
      .venue strong {
          color: #99324b;
      }

      .benchmark {
          width: 100%;
          max-width: 960px;
          overflow: scroll;
          overflow-y: hidden;
      }

      .simple {
          color:#52adc8 ;
      }
    </style>
  </head>
  <body>

    <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="container">
      <h4 style="text-align:center">GLASS: Geometric Latent Augmentation for Shape Spaces</h4>
      <p align="center", style="margin-bottom:12px;">
        <a class="simple" href="https://sanjeevmk.github.io/">Sanjeev Muralikrishnan</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://noamaig.github.io">Noam Aigerman</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="http://www.vovakim.com">Vladimir Kim</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://techmatt.github.io">Matthew Fisher</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="http://www0.cs.ucl.ac.uk/staff/n.mitra/">Niloy J. Mitra</a><sup>1</sup>,<sup>2</sup>
      </p>

      <p align="center" style="margin-bottom:20px;">
        <sup>1</sup>University College London
        <span style="display:inline-block; width: 32px"></span>
        <sup>2</sup>Adobe Research <br>
      </p>

      <div class="venue">
        <p align="center"> Computer Vision And Pattern Recognition (CVPR), 2022 </p>
      </div>

      <figure>
        <img src="images/teaser.png" style="width:100%"></img>
        <!-- <br> -->
      </figure>
      <div class="caption">
        Starting from just 10 shapes (larger), our method iteratively augments the collection by alternating between training a VAE, and exploring random perturbations in its low-dimensional latent space guided by a purely geometric deformation energy. Here we show the 1000 most diverse shapes from the first 5K discovered by our method, positioned according to their latent embedding (projected to 2D via t-SNE). Shapes are colored according to the initial landmark they trace back to, with shapes added in later iterations lighter (greyer) in color. The augmentation effectively fills in the space between the sparse initial landmarks, and even extrapolates beyond them. It manages to also interpolate global rotations for samples near the back-facing exemplar, and yields shapes with larger feet-strides (far left), and crossed arms or feet (front, left and center) even though there are no such initial landmarks.
      </div>

      <br><br>

      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        <h5>Abstract</h5>
        <p align="justify">
            We investigate the problem of training generative models on very sparse collections of 3D models. Particularly, instead of using difficult-to-obtain large sets of 3D models, we demonstrate that geometrically-motivated energy functions can be used to effectively augment and boost only a sparse collection of example (training) models. Technically, we analyze the Hessian of the as-rigid-as-possible (ARAP) energy to adaptively sample from and project to the underlying (local) shape space, and use the augmented dataset to train a variational autoencoder (VAE). We iterate the process, of building latent spaces of VAE and augmenting the associated dataset, to progressively reveal a richer and more expressive generative space for creating geometrically and semantically valid samples. We extensively evaluate our method against a set of strong baselines, provide ablation studies, and demonstrate application towards establishing shape correspondences. GLASS produces multiple interesting and meaningful shape variations even when starting from as few as 3-10 training shapes. 
          <br>
          <br>
        </p>
      </div>
      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        <h5>Video</h5>
      <center>
          <video width="560" height="315" controls>
            <source src="video/glass_dist.mp4" type="video/mp4">
          </video>

      <!--<iframe width="560" height="315" src="https://www.youtube.com/embed/u_8DJ06SQdw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>-->
      </center>
      <br>
      </div>
    <div class="section">
        <h5>Method Pipeline</h5>
        <center>
          <img src="images/overview_glass.png" style="width:100%"></img>
            <div class="caption" >
              <p align="justify">
                We present GLASS to iteratively build a deformation-aware VAE latent space and analyzing it to generate new training samples to augment the original training set. This enables generation of diverse yet plausible shape variations starting from very few input examples. 
              </p>
            </div>
            <br>
        </center>
    </div>

    <div class="section">
        <h5>Materials</h5>
        <div class="container" style="width:95%">
          <!-- Icon row -->
          <div class="row">
            <div class="two columns">
              <a href="https://arxiv.org/pdf/2108.03225.pdf"><img style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; width: 108px;" src="images/paper_shot.png"></a>
            </div>
          </div>
          <!-- Link row -->
          <div class="row">
            <div class="two columns">
              <a href="https://arxiv.org/pdf/2108.03225.pdf">Paper</a>
            </div>
          </div>

          <div class="row">
              <br>
                     <p>Code to be released soon!</p>
          </div>

        </div>
      </div>

  <br>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Qualitative Results</h5>
    <center>
      <img src="images/generation.png" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            Training GLASS on the human, centaur, and horse meshes using the 3 examples each (top). (Bottom) We show random samples from the latent space, which combine different properties learned from the example deformations.
          </p>
        </div>
        <br>
    </center>
    <center>
      <img src="images/face_generation_3.png" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            Despite facial expressions not being perfectly locally rigid, we show above that GLASS generates plausible novel expressions on the COMA dataset from just 3 landmarks.
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/face_generation.png" style="width:100%"></img>
        <div class="caption" >
          <p align="center">
            GLASS's facial-expression generation results, when trained on 6 expressions from the COMA dataset.
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/faust3_fat.png" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            In the above figure, we show that GLASS can work with any bodyshape, with generated samples (gold) from 3 FAUST poses (grey) of a large-frame identity.
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/retrieval.png" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            Generation results evaluated by coverage. We train different methods on the same training data (col 1) and generate
comparable numbers of shapes. Given two shapes from the holdout data (col 2), we evaluate the methods by finding the closest generated shape (cols 3-9). Note how the baselines exhibit strong artifacts and usually do not match the query shape
          </p>
        </div>
        <br>
    </center>
    <center>

      <img src="images/interpolation_comparison.png" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            Interpolation results. In gray, we show two landmark shapes. In gold, we show the decoded meshes after we linearly interpolate the latent space between these two landmarks. All models are trained on only 5 landmarks.
          </p>
        </div>
        <br>
    </center>
    <center>

      <img src="images/interpolation_comparison_raster.png" style="width:100%"></img>
        <div class="caption" >
          <p align="center">
            We compare the interpolation results between our method, several ablations of our method, and prior work.
          </p>
        </div>
        <br>
    </center>
    <center>

      <img src="images/latent_expansion.png" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            tSNE embedding of generated samples shows progressive augmentation of the shape space. Sample color indicates originating (parent) shape.
          </p>
        </div>
        <br>
    </center>

  </div>
  
	<div class="section">
          <h5>Citation</h5> 
  <pre style="margin:0"><code>
  @article{DBLP:journals/corr/abs-2108-03225,
  author    = {Sanjeev Muralikrishnan and
               Siddhartha Chaudhuri and
               Noam Aigerman and
               Vladimir G. Kim and
               Matthew Fisher and
               Niloy J. Mitra},
  title     = {{GLASS:} Geometric Latent Augmentation for Shape Spaces},
  journal   = {CoRR},
  volume    = {abs/2108.03225},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.03225},
  eprinttype = {arXiv},
  eprint    = {2108.03225},
  timestamp = {Wed, 11 Aug 2021 15:24:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-03225.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
  </code></pre>    			
		</div>

    <!--

    -->

        
  </div>

    <script type="text/javascript" src="../js/jquery.min.js"></script>
    <script type="text/javascript" src="../js/footable.min.js"></script>

    <script type="text/javascript">
      jQuery(function($){
          $('.table').footable();
      });
    </script>

    <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  </body>
</html>