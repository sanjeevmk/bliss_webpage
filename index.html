<!DOCTYPE html>
<html lang="en">
  <head>
    <br>
    <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta charset="utf-8">
    <title>BLiSS: Bootstrapped Linear Shape Space</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
         <link rel="preconnect" href="https://fonts.googleapis.com"> 
         <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
         <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&display=swap" rel="stylesheet">
    <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/footable.standalone.min.css">

    <!-- Favicon
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="icon" type="image/png" href="images/favicon.png">

    <!-- Google icon -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

    <!-- Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                               m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-86869673-1', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
    <style>
      img {
          display: block;
      }

      .column-50 {
          float: left;
          width: 50%;
      }
      .row-50:after {
          content: "";
          display: table;
          clear: both;
      }

      .floating-teaser {
          float: left;
          width: 30%;
          text-align: center;
          padding: 15px;
      }
      .venue strong {
          color: #99324b;
      }

      .benchmark {
          width: 100%;
          max-width: 960px;
          overflow: scroll;
          overflow-y: hidden;
      }

      .simple {
          color:#52adc8 ;
      }
    </style>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
      src="./js/MathJax.js">
    </script>
  </head>
  <body>

    <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="container">
      <h4 style="text-align:center">BLiSS: Bootstrapped Linear Shape Space</h4>
      <p align="center", style="margin-bottom:12px;">
        <a class="simple" href="https://sanjeevmk.github.io/">Sanjeev Muralikrishnan</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://www.duygu-ceylan.com/">Duygu Ceylan</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://is.mpg.de/~chuang2">Chun-Hao Paul Huang</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="http://www0.cs.ucl.ac.uk/staff/n.mitra/">Niloy J. Mitra</a><sup>1</sup>,<sup>2</sup>
      </p>

      <p align="center" style="margin-bottom:20px;">
        <sup>1</sup>University College London
        <span style="display:inline-block; width: 32px"></span>
        <sup>2</sup>Adobe Research <br>
      </p>

      <div class="venue">
        <p align="center"> </p>
      </div>

      <figure>
        <img src="new_images/teaser_v2_2rows.png" style="width:100%"></img>
        <!-- <br> -->
      </figure>
      <div class="caption">
        We present BLiSS, a method that progressively builds a human body shape space and brings unregistered scans into correspondence to a given template. Starting from as few as 200 manually registered scans (green samples), BLiSS creates an expressive shape space (pink samples), performing on-par with state-of-the-art models such as SMPL, STAR, and GHUM. (Right) Our space can be used to recover the body-shape parameters of raw scans by projecting them directly to ours
      </div>

      <br><br>

      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        <h5>Abstract</h5>
        <p align="justify">
            Morphable models are a backbone for many human-centric workflows as they provide a simple yet expressive
            shape space. Creating such morphable models, however, is
            both tedious and expensive. The main challenge is to carefully establish dense correspondences among raw scans
            that capture sufficient shape variation. This is often addressed using a mix of non-rigid registration and significant manual intervention. We observe that creating a shape
            space and solving for dense correspondence are tightly coupled – while dense correspondence is needed to build shape
            spaces, an expressive shape space can provide a reduced
            dimensional space to regularize the search. We introduce
            BLiSS, a method to solve both progressively. Starting from a
            small set of manually registered scans to bootstrap the process, we simultaneously enrich the shape space and then
            use that to automatically get new unregistered scans into
            correspondence. The critical component of BLiSS is a non-linear deformation model that captures details missed by
            the low-dimensional shape space, thus allowing progressive
            enrichment of the space. We show that ours produces, in the
            context of body variations, a shape space that is at par with
            state-of-the-art shape spaces (e.g., SMPL, STAR, GHUM),
            while requiring much fewer (e.g., 5%) manual registrations
          <br>
          <br>
        </p>
      </div>
    <div class="section">
        <h5>Materials</h5>
        <div class="container" style="width:100%">
          <!-- Icon row -->
          <div class="row">
            <div class="two columns">
              <a href="./resources/BLiSS.pdf"><img style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; width: 108px;" src="new_images/paper_shot.png"></a>
            </div>
            <div class="two columns">
              <a href=""><img style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; width: 108px;" src="images/github.png"></a>
            </div>
          </div>
          <!-- Link row -->
          <div class="row">
            <div class="two columns">
              <a href="./resources/BLiSS.pdf">Paper</a>
            </div>
            <div class="two columns">
              <!-- <a href="https://github.com/sanjeevmk/glass/">Code</a> -->
              TBA
            </div>
          </div>

        </div>
      </div>

  <br>

    <div class="section">
        <h5>Method Pipeline</h5>
        <center>
          <img src="new_images/pipeline.png" style="width:100%"></img>
            <div class="caption" >
              <p align="justify">
                Given a sparse set of scans $S_R$, and their registrations $R$ to a common template, we learn a linear shape space
$B_{PCA}$ using $R_{PCA}$ and train a non-linear NJF based deformation model using $R_{DEFORM}$. Given a scan $S_U$ from a set
of unregistered scans $U$, we first project it to the PCA basis to obtain $X_o$ and utilize NJF-based deformation to recover its
registration to the template $X′$ in the canonical pose. We compute the Chamfer Distance ($D_{CD}$) of the registrations to the
target scans and add all the registrations where the distance is within one standard deviation of the minimum distance to
RPCA and update our shape space. By repeating these steps, we jointly register raw scans and enhance our shape space.
              </p>
            </div>
            <br>
        </center>
    </div>


  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Results</h5>
    <center>
      <img src="images/histogram_progression.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            We show the histogram of the v2v error of the scans in our test set at different iterations of our method. We also color code the per-vertex error for an example scan. As our method progresses, the error decreases, and we observe a slight left shift in the histogram as the shape space improves. Insets show residue error on one scan over iterations.
          </p>
        </div>
        <br>
    </center>
    <center>
      <img src="images/registration_comparison_v2.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            For a given raw scan, we register each body model by predicting pose and body shape parameters. (Top) Each result is color coded based on the v2p error in meters w.r.t.~the ground truth registration provided by the artist.
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/modes.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="center">
            We show shapes along the top three principal directions in different rows, and observe variations in gender, height, and weight along the respective PCA  modes.
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/body_and_face.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            <i>Left:</i>Registration (pink) of noisy scans (blue) with our final shape space. Since our model does not capture finger-level details, after optimization, the joints corresponding to the greyed-out regions are reset to default poses. <i>Right:</i>We show sampled faces from our final face-shape space after growing it from 20 to 800 shapes. We observe a variety of face changes in the cheek and nose regions. (Bottom) We take the test scans from the COMA dataset (in blue) and register them in our final face-shape space, which is shown in pink.
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/smplify.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            Here we use the trained SMPLify-X model to estimate the shape from a single image. For BLiSS, we plugin our shape space as a drop-in replacement for SMPL's space, while using SMPL's pose space.
          </p>
        </div>
        <br>
    </center>
    <center>

      <img src="images/bliss_face_rounds.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="justify">
            <b>Iterative Shape Corrections:</b> Lightly colored faces in the middle are our registrations in earlier iterations of the space, and the pink-colored face on the right is our registration after five rounds of BLiSS. As the rounds progress, registrations in later rounds more accurately capture the scan (left, in Blue), as observed by the broadening of the nose and jawline.
          </p>
        </div>
        <br>
    </center>
    <center>

      <img src="images/mano_hands.jpg" style="width:100%"></img>
        <div class="caption" >
          <p align="center">
            <b>Registering Hand scans:</b> We use our final hand-shape space to register hand scans (blue); registrations in canonical pose (i.e., default) shown in pink.
          </p>
        </div>
        <br>
    </center>
  </div>

  <!--
	<div class="section">
          <h5>Citation</h5> 
  <pre style="margin:0"><code>
@inproceedings{glass2022sanjeev,
  title={GLASS: Geometric Latent Augmentation for Shape Spaces},
  author={Muralikrishnan, Sanjeev and Chaudhuri, Siddhartha and Aigerman, Noam and Kim, Vladimir and Fisher, Matthew and Mitra, Niloy},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
  </code></pre>    			
		</div>
  -->

    <!--

    -->

        
  </div>

    <script type="text/javascript" src="./js/jquery.min.js"></script>
    <script type="text/javascript" src="./js/footable.min.js"></script>

    <script type="text/javascript">
      jQuery(function($){
          $('.table').footable();
      });
    </script>

    <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  </body>
</html>
